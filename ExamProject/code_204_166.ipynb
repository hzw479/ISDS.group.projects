{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c11778fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mieharder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mieharder/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mieharder/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re # python regular expressions\n",
    "import string # for efficient operations with strings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('punkt') # you will probably need to do this\n",
    "nltk.download('wordnet') # and this\n",
    "nltk.download('stopwords') # aand this\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from vaderSentiment import vaderSentiment\n",
    "from afinn import Afinn\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf315aa",
   "metadata": {},
   "source": [
    "## Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell will save name and ranking of American Universities from US News official \n",
    "    ranking 2021 in a dataframe. From here, the cell randomly chooses 10 universities \n",
    "    from the bottom of the list and prints them. \"\"\"\n",
    "\n",
    "ranking=[]\n",
    "name=[]\n",
    "\n",
    "# Create a request interceptor\n",
    "def interceptor(request):\n",
    "    del request.headers['Referer']  # Delete the header first\n",
    "    request.headers['Referer'] = 'For an exam at the university of copenhagen for the course ISDS'\n",
    "\n",
    "# Set the interceptor on the driver\n",
    "\n",
    "#Få den første\n",
    "for j in range(round(338/10)+1):\n",
    "    url = ('https://www.usnews.com/best-colleges/rankings/national-universities?_page=' + str(j+1))\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.request_interceptor = interceptor\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    driver.close()\n",
    "    clasees = soup.findAll('div', {'class':'Box-w0dun1-0 DetailCardColleges__TextContainer-cecerc-4 CYZBT eoCwZT'})\n",
    "    class_name = soup.findAll('a', {'class':'Anchor-byh49a-0 DetailCardColleges__StyledAnchor-cecerc-7 fidpEI eKrerU card-name'})\n",
    "\n",
    "    for i in range(len(clasees)):\n",
    "        temp = clasees[i].text\n",
    "        ranking.append(re.findall('\\d+', temp)[0])\n",
    "        name.append(class_name[i].text.strip())\n",
    "\n",
    "df = pd.DataFrame(list(zip(ranking, name)),\n",
    "               columns =['Ranking', 'Name'])\n",
    "\n",
    "\n",
    "#Get subset with ranking 298 (there are 92)\n",
    "bottom_df = df[df[\"Ranking\"] == '298']\n",
    "\n",
    "#Draw a random 10\n",
    "ten_bottom = bottom_df.sample(10)\n",
    "\n",
    "print(ten_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ec0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_schools={'Princeton University':{'Number of professors':326 , 'School ID':780},\n",
    "                 'Harvard University':{'Number of professors': 567, 'School ID':399},\n",
    "                 'Columbia University':{'Number of professors': 893, 'School ID':278},\n",
    "                 'Massachusetts Institute of Technology':{'Number of professors':346 , 'School ID':580},\n",
    "                 'Yale University':{'Number of professors':464 , 'School ID':1222},\n",
    "                 'Stanford University':{'Number of professors':611 , 'School ID':953},\n",
    "                 'University of Chicago':{'Number of professors':507 , 'School ID':1085},\n",
    "                 'University of Pennsylvania':{'Number of professors':523 , 'School ID':169},\n",
    "                 'California Institute of Technology':{'Number of professors':93 , 'School ID':148},\n",
    "                 'Johns Hopkins University':{'Number of professors':830 , 'School ID':464},\n",
    "                 \n",
    "                 'Wichita State University':{'Number of professors':1713 , 'School ID':1197},\n",
    "                 'Western Kentucky University':{'Number of professors':1669 , 'School ID':1176},\n",
    "                 'University of Charleston':{'Number of professors':94 , 'School ID':1084},\n",
    "                 'Regent University':{'Number of professors':485 , 'School ID':4375},\n",
    "                 'University of Hawaii at Hilo ':{'Number of professors':435 , 'School ID':1105},\n",
    "                 'Cleveland State University ':{'Number of professors':1862 , 'School ID':244},\n",
    "                 'Husson University':{'Number of professors':221 , 'School ID':426},\n",
    "                 'Palm Beach Atlantic University':{'Number of professors':357 , 'School ID':753},\n",
    "                 'Texas Womans University':{'Number of professors':1311 , 'School ID':1014},\n",
    "                 'University of Texas at Tyler':{'Number of professors':717 , 'School ID':4171},\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d536fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(school_id, number_of_professors):\n",
    "    \"\"\"\n",
    "    args: school_id: integer\n",
    "          number_of_professors: integer\n",
    "    returns: None\n",
    "    \n",
    "    This function takes as an argument, a school ID and the number of professors at the corresponding school.\n",
    "    It then reads the URL corresponding the school and saves a dataframe containing information of the given school\n",
    "    and their professors into a csv file.\n",
    "    From the dataframe it makes a list of teacher ID's, which are used to access all professors RMP page and scraping\n",
    "    comments regarding each professor.\n",
    "    Lastly it saves the comments in a second dataframe, which is then saved as a csv file.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_pages=math.ceil(number_of_professors/20)\n",
    "    list_of_dfs=[]\n",
    "    for j in range(1,number_of_pages+1): #all page numbers\n",
    "        url=f\"https://www.ratemyprofessors.com/filter/professor/?&page={j}&filter=teacherlastname_sort_s+asc&query=*%3A*&queryoption=TEACHER&queryBy=schoolId&sid=\"+str(school_id)\n",
    "        response = requests.get(url) #this response is a dict of dicts.\n",
    "        list_of_dicts=response.json() #keys are: ['professors', 'searchResultsTotal', 'remaining', 'type']\n",
    "        list_of_professor_dicts=list_of_dicts['professors'] #enter the dict with info regarding professors\n",
    "        list_of_dfs.append(pd.concat([pd.DataFrame(list_of_dicts['professors'] [i], index=[i]) for i in range(len(list_of_professor_dicts))],\n",
    "                  ignore_index=True)) #appends a dataframe with information of all professors on the given page to list\n",
    "    df=pd.concat([list_of_dfs[i] for i in range(len(list_of_dfs))], axis= 0).reset_index(drop=True) #after going through all pages. Concatenate all df's into one\n",
    "    list_of_teacher_ids=list(df.tid) #create a list of all professor id's for the given university\n",
    "    list_of_comments_for_each_teacher=[]\n",
    "    list_of_tags_for_each_teacher=[]\n",
    "    for i in range(len(list_of_teacher_ids)):\n",
    "        my_url = f'https://www.ratemyprofessors.com/ShowRatings.jsp?tid={list_of_teacher_ids[i]}'#enter URL for each professor\n",
    "        my_response = requests.get(my_url)\n",
    "        html = my_response.text\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        my_text = soup.findAll('div', {'class':'Comments__StyledComments-dzzyvm-0 gRjWel'})#list of comments unparsed\n",
    "        my_tags = soup.findAll('span', {'class':'Tag-bs9vf4-0 hHOVKF'})#list of comments unparsed\n",
    "        finished_comments=[i.text for i in my_text]\n",
    "        finished_tags=[i.text for i in my_tags]\n",
    "        list_of_comments_for_each_teacher.append(finished_comments)\n",
    "        list_of_tags_for_each_teacher.append(finished_tags)\n",
    "    df['Comments']=list_of_comments_for_each_teacher #add a column to dataframe where each cell contains a list of all comments for the given professor.\n",
    "    df['Tags']=list_of_tags_for_each_teacher#add a column to dataframe where each cell contains a list of all tags for the given professor.\n",
    "    df.to_csv(f'{school_id}.csv') #saves the dataframe as a csv file\n",
    "    \n",
    "    ##Second dataframe\n",
    "    list_of_comments_and_ids=[]\n",
    "    for i in range(len(list_of_teacher_ids)):\n",
    "        my_url = f'https://www.ratemyprofessors.com/ShowRatings.jsp?tid={list_of_teacher_ids[i]}'\n",
    "        my_response = requests.get(my_url, headers={'user-agent': 'For an exam at the university of copenhagen for the course ISDS'})\n",
    "        html = my_response.text\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        my_comments = soup.findAll('div', {'class':'Comments__StyledComments-dzzyvm-0 gRjWel'})#list of tags unparsed\n",
    "        my_date = soup.findAll('div', {'class':'TimeStamp__StyledTimeStamp-sc-9q2r30-0 bXQmMr RatingHeader__RatingTimeStamp-sc-1dlkqw1-3 BlaCV'})#list of dates unparsed\n",
    "        my_quality = soup.findAll('div', {'class':'CardNumRating__StyledCardNumRating-sc-17t4b9u-0 eWZmyX'})#list of dates unparsed\n",
    "        finished_quality=[i.text for i in my_quality]\n",
    "        finished_comments=[i.text for i in my_comments]\n",
    "        finished_dates=[i.text for i in my_date]\n",
    "        if len(finished_comments)!=0:\n",
    "            list_of_comments_and_ids.append([finished_comments, [k[-3:] for k in finished_quality[0::2]],[k[-3:] for k in finished_quality[1::2]], finished_dates[0::2],[list_of_teacher_ids[i] for j in range(len(finished_comments))]])\n",
    "    hej=pd.concat([pd.DataFrame(list_of_comments_and_ids[i]).T for i in range(len(list_of_comments_and_ids))]) #creates the dataframe\n",
    "    hej.columns=['Comments', 'Quality','Difficulty','Dates', 'TeacherID']\n",
    "    hej=hej.reset_index(drop=True)\n",
    "    hej.to_csv(f'{school_id}comments.csv')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a424b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For running the get_all function for all universities. Be ware that running this takes several hours.\n",
    "\n",
    "for i in list(dict_of_schools.keys()):\n",
    "    print(i)\n",
    "    school_to_run=i\n",
    "    number_of_prof=dict_of_schools[school_to_run]['Number of professors']\n",
    "    schoolID=dict_of_schools[school_to_run]['School ID']\n",
    "    get_all(schoolID, number_of_prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adc30b",
   "metadata": {},
   "source": [
    "## Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd2b8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinates all professor dataframes to one and saves as csv file\n",
    "all_df=[]\n",
    "for i in list(dict_of_schools.keys()):\n",
    "    nr = dict_of_schools[i]['School ID']\n",
    "    df = pd.read_csv(f'initialcsvfiles/{nr}.csv')\n",
    "    df['SchoolID'] = nr\n",
    "    all_df.append(df)\n",
    "    \n",
    "df_all = pd.concat(all_df)\n",
    "df_all.to_csv('concatinated_df')\n",
    "\n",
    "\n",
    "#Concatinates all comment dataframes to one and saves as csv file\n",
    "all_df=[]\n",
    "for i in list(dict_of_schools.keys()):\n",
    "    nr = dict_of_schools[i]['School ID']\n",
    "    df = pd.read_csv(f'initialcsvfiles/{nr}comments.csv')\n",
    "    df['SchoolID'] = nr\n",
    "    all_df.append(df)\n",
    "    \n",
    "df_all = pd.concat(all_df)\n",
    "df_all.to_csv('concatinated_comments_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43550a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of top and bottom school numbers, to categorize out schools in dataframe.\n",
    "top = []\n",
    "bottom = []\n",
    "\n",
    "for i in range(len(list(dict_of_schools.keys()))):\n",
    "    if i < 10:\n",
    "        top.append(dict_of_schools[list(dict_of_schools.keys())[i]]['School ID'])\n",
    "    else:\n",
    "        bottom.append(dict_of_schools[list(dict_of_schools.keys())[i]]['School ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4db39f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_professor=pd.read_csv('concatinated_df')\n",
    "concat_comment=pd.read_csv('concatinated_comments_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2dd609a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data and saves final df as csv\n",
    "df = concat_professor.drop(['contentType', 'categoryType', 'Unnamed: 0.1', 'Unnamed: 0', 'tLname', 'tMiddlename', 'tFname', 'SchoolID' ], axis =1)\n",
    "df['status'] = np.where(df['tSid'].isin(top), 'top', 'bottom') #adds a status column\n",
    "df = df.dropna() #drops rows with NaN values\n",
    "df.to_csv('df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6763d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data and saves final df as csv\n",
    "df_no_comment = df[df['Comments'] == '[]'] \n",
    "df_comments = concat_comment.loc[~concat_comment['TeacherID'].isin(list(df_no_comment['tid']))]\\\n",
    "         .drop(['Unnamed: 0.1', 'Unnamed: 0' ], axis =1)#removes all rows where the teacher has no reviews in professor df\n",
    "df_comments['Dates'] = pd.to_datetime(df_comments.Dates) #convert dates to datetime format\n",
    "df_comments['status'] = np.where(df_comments['SchoolID'].isin(top), 'top', 'bottom') #adds a status column\n",
    "df_comments = df_comments[df_comments['Comments'] != 'No Comments'] #removes no comments\n",
    "df_comments.dropna(inplace=True) #remove rows with NaN \n",
    "df_comments.to_csv('df_comments_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e19dc",
   "metadata": {},
   "source": [
    "## Working with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95df637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the dataframes\n",
    "df_p = pd.read_csv('df_final.csv')\n",
    "df_c = pd.read_csv('df_comments_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9111718",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('STOPlist.txt') as f: #note that we made our own stop words list\n",
    "    stoppelop = f.read().splitlines() #list of stop words\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Function to lemmatize and tokenize  text\"\"\"\n",
    "    low_text= text.lower()\n",
    "    low_text = low_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(low_text)\n",
    "    porter = nltk.WordNetLemmatizer()\n",
    "    lemmatizer=[porter.lemmatize(t) for t in tokens]\n",
    "    stop_words_list = stoppelop\n",
    "    sent_sw_removed = [i for i in lemmatizer if i not in stop_words_list]\n",
    "    lemmas=[i for i in sent_sw_removed if i!='br']\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbc1e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make column with list of lemmatized and tokenized comments\n",
    "df_c['Clean_comment']=df_c.apply(lambda row: preprocess(row.Comments), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d4b71",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8694c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df_c['status']\n",
    "\n",
    "X = df_c['Comments']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=161193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27adea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer\n",
    "vectorizerc = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "X_train_c = vectorizerc.fit_transform(X_train)\n",
    "X_test_c = vectorizerc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ddc4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF vectorizer\n",
    "vectorizert = TfidfVectorizer(tokenizer=preprocess)\n",
    "\n",
    "X_train_tf = vectorizert.fit_transform(X_train)\n",
    "X_test_tf = vectorizert.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f38eb",
   "metadata": {},
   "source": [
    "### Lasso Regression word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa8cee13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8260138065008499\n",
      "testing accuracy: 0.7912373816213966\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "lr = LogisticRegression(random_state=0, penalty = 'l1', solver = 'saga', max_iter=4000)\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_c,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_c)\n",
    "test_preds = lr.predict(X_test_c)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f659d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0\n",
      "backtests         5.118388\n",
      "declared          3.449085\n",
      "inferential       3.191841\n",
      "remarkably        3.135088\n",
      "eloquent          2.916783\n",
      "redemption        2.870802\n",
      "recommed          2.792675\n",
      "nobel             2.791912\n",
      "recycled          2.625961\n",
      "researeasch       2.592687\n",
      "testsassignments  2.555164\n",
      "sucess            2.546583\n",
      "pset              2.444369\n",
      "australian        2.394399\n",
      "superfluous       2.365215\n",
      "9am               2.327386\n",
      "neuroscience      2.322469\n",
      "coursera          2.308388\n",
      "persian           2.269973\n",
      "mistaken          2.261013\n",
      "xcredit           2.253713\n",
      "catchup           2.244231\n",
      "caltech           2.209440\n",
      "mentorship        2.205322\n",
      "legendary         2.183779\n",
      "arrogent          2.177932\n",
      "jam               2.175048\n",
      "china             2.174193\n",
      "ditsy             2.163920\n",
      "900               2.159900\n",
      "smoked            2.154484\n",
      "relevance         2.146844\n",
      "expo20            2.130280\n",
      "gd                2.129583\n",
      "quarter           2.127280\n",
      "summarized        2.127201\n",
      "opaque            2.114269\n",
      "1802              2.097430\n",
      "laminate          2.087559\n",
      "availability      2.077975\n",
      "1803              2.061171\n",
      "intensely         2.060218\n",
      "cmd               2.056934\n",
      "devotes           2.052080\n",
      "classwide         2.051857\n",
      "cow               2.041050\n",
      "distributes       2.031179\n",
      "hesitant          2.022855\n",
      "michael           2.020453\n",
      "manditory         2.013229 \n",
      "                       0\n",
      "pertaining    -2.982723\n",
      "lawyer        -2.679188\n",
      "preaches      -2.201696\n",
      "hybrid        -1.866316\n",
      "snack         -1.813245\n",
      "purchase      -1.777940\n",
      "smallest      -1.760918\n",
      "breath        -1.741871\n",
      "blog          -1.608357\n",
      "19            -1.561195\n",
      "simulation    -1.561045\n",
      "supplemental  -1.545988\n",
      "resourceful   -1.543320\n",
      "edit          -1.533393\n",
      "flowing       -1.506764\n",
      "span          -1.499916\n",
      "calendar      -1.452923\n",
      "intersting    -1.450783\n",
      "retired       -1.441156\n",
      "caution       -1.436575\n",
      "vital         -1.427760\n",
      "retained      -1.412520\n",
      "flake         -1.411136\n",
      "presession    -1.406694\n",
      "plug          -1.395686\n",
      "aggressive    -1.392595\n",
      "awfully       -1.380010\n",
      "blessing      -1.375631\n",
      "replying      -1.349035\n",
      "informational -1.339471\n",
      "211           -1.338352\n",
      "opening       -1.334789\n",
      "slideshows    -1.332104\n",
      "electronic    -1.330542\n",
      "liking        -1.325351\n",
      "incorrectly   -1.317006\n",
      "copied        -1.312047\n",
      "survival      -1.307906\n",
      "replace       -1.306604\n",
      "biterm        -1.305424\n",
      "forgetful     -1.281587\n",
      "canvas        -1.264904\n",
      "softly        -1.251653\n",
      "snarky        -1.226757\n",
      "hoot          -1.223222\n",
      "awkward       -1.220649\n",
      "choir         -1.218514\n",
      "appealing     -1.207050\n",
      "511           -1.196633\n",
      "illness       -1.194306\n"
     ]
    }
   ],
   "source": [
    "#Prints highest and lowest weighted features with their weights\n",
    "features = ['_'.join(s.split()) for s in vectorizerc.get_feature_names()]\n",
    "coefs_df = pd.DataFrame.from_records(lr.coef_, columns=features)\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=False).head(50), '\\n',coefs_df.T.sort_values(by=[0], ascending=True).head(50) )\n",
    "\n",
    "coefs_df.to_csv(\"coef_final.csv\") #Saves the coefficients into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e287111",
   "metadata": {},
   "source": [
    "### Lasso Regression with TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce9151eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8087383355881639\n",
      "testing accuracy: 0.7955042147986263\n"
     ]
    }
   ],
   "source": [
    "# classifier - \n",
    "lr = LogisticRegression(random_state=0, penalty = 'l1', solver = 'saga')\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_tf,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_tf)\n",
    "test_preds = lr.predict(X_test_tf)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c7f09",
   "metadata": {},
   "source": [
    "### Ridge Regression word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "da1b81c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8507995975994728\n",
      "testing accuracy: 0.7933707982100114\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "lr = LogisticRegression(random_state=0, penalty = 'l2', solver = 'saga', max_iter=4000)\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_c,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_c)\n",
    "test_preds = lr.predict(X_test_c)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36de85",
   "metadata": {},
   "source": [
    "### Further statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5eca0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make column with length of comments\n",
    "df_c['len_col'] = df_c['Comments'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4e0d29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   status   Quality\n",
      "0  bottom  1.476532\n",
      "1     top  1.486613\n",
      "   status  Difficulty\n",
      "0  bottom    1.246345\n",
      "1     top    1.235679\n",
      "   status  Difficulty\n",
      "0  bottom    3.058642\n",
      "1     top    3.084285\n",
      "   status   Quality\n",
      "0  bottom  3.649149\n",
      "1     top  3.762763\n",
      "   status     len_col\n",
      "0  bottom  240.231233\n",
      "1     top  189.453689\n"
     ]
    }
   ],
   "source": [
    "print(df_c.groupby('status', as_index=False)['Quality'].std())\n",
    "print(df_c.groupby('status', as_index=False)['Difficulty'].std())\n",
    "\n",
    "print(df_c.groupby('status', as_index=False)['Difficulty'].mean())\n",
    "print(df_c.groupby('status', as_index=False)['Quality'].mean())\n",
    "\n",
    "print(df_c.groupby('status', as_index=False)['len_col'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
