{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cfcb7c",
   "metadata": {},
   "source": [
    "## Regressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8ce43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Liv\n",
      "[nltk_data]     Nøhr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Liv\n",
      "[nltk_data]     Nøhr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Liv\n",
      "[nltk_data]     Nøhr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## load packages\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#For word handeling\n",
    "import string \n",
    "import nltk\n",
    "nltk.download('punkt') # you will probably need to do this\n",
    "nltk.download('wordnet') # and this\n",
    "nltk.download('stopwords') # aand this\n",
    "\n",
    "# for classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcc42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    low_text= text.lower()\n",
    "    low_text = low_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(low_text)\n",
    "    porter = nltk.WordNetLemmatizer()\n",
    "    lemmatizer=[porter.lemmatize(t) for t in tokens]\n",
    "    stop_words_list = stoppelop\n",
    "    sent_sw_removed = [i for i in lemmatizer if i not in stop_words_list]\n",
    "    lemmas=[i for i in sent_sw_removed if i!='br']\n",
    "    return lemmas # return a list of stems/lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89119c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data\n",
    "df_c = pd.read_csv('df_comments_final.csv')\n",
    "df_c = df_c.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "df_c=df_c[df_c['Comments']!='No Comments']\n",
    "\n",
    "df_c['year'] = df_c['Dates'].str[2:4]\n",
    "\n",
    "#Divide into target and features\n",
    "y= df_c['status']\n",
    "\n",
    "X = df_c['Comments']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=161193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82632712",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('STOPlist.txt') as f:\n",
    "    stoppelop = f.read().splitlines()\n",
    "#stoppelop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b805f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655.4979617595673\n"
     ]
    }
   ],
   "source": [
    "#Vectorize - both using count and tfidf\n",
    "\n",
    "start = time.time()\n",
    "#using count and own preprocesser\n",
    "vectorizerc = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# The top N most frequent features:\n",
    "#vectorizerc_most = CountVectorizer(max_features=80, tokenizer=preprocess)\n",
    "\n",
    "#using tfidf vectorizer\n",
    "#vectorizertfidf = TfidfVectorizer(tokenizer=preprocess)\n",
    "\n",
    "\n",
    "#Using tfidf vectorizer w highest features\n",
    "#vectorizertfidf = Tfidf_mostVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# fit and transform train and test set for each vectorizer:\n",
    "X_train_c = vectorizerc.fit_transform(X_train)\n",
    "#X_train_c_most = vectorizerc_most.fit_transform(X_train)\n",
    "#X_train_tf = vectorizertfidf.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Only tranform test set: never fit your vectorizer on the test set (it is cheating). Out-of-Vocabulary words are handled automatically be sklearn's vectorizer.\n",
    "X_test_c = vectorizerc.transform(X_test)\n",
    "#X_test_bow = vectorizerc_most.transform(X_test)\n",
    "#X_test_tf = vectorizertfidf.transform(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76619508",
   "metadata": {},
   "source": [
    "### Check difference in precision of count and tf_idf vectorization (using lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7905183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8260138065008499\n",
      "testing accuracy: 0.7912373816213966\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "lr = LogisticRegression(random_state=0, penalty = 'l1', solver = 'saga', max_iter=4000)\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_c,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_c)\n",
    "test_preds = lr.predict(X_test_c)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243a008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0\n",
      "backtests         5.118388\n",
      "declared          3.449085\n",
      "inferential       3.191841\n",
      "remarkably        3.135088\n",
      "eloquent          2.916783\n",
      "redemption        2.870802\n",
      "recommed          2.792675\n",
      "nobel             2.791912\n",
      "recycled          2.625961\n",
      "researeasch       2.592687\n",
      "testsassignments  2.555164\n",
      "sucess            2.546583\n",
      "pset              2.444369\n",
      "australian        2.394399\n",
      "superfluous       2.365215\n",
      "9am               2.327386\n",
      "neuroscience      2.322469\n",
      "coursera          2.308388\n",
      "persian           2.269973\n",
      "mistaken          2.261013\n",
      "xcredit           2.253713\n",
      "catchup           2.244231\n",
      "caltech           2.209440\n",
      "mentorship        2.205322\n",
      "legendary         2.183779\n",
      "arrogent          2.177932\n",
      "jam               2.175048\n",
      "china             2.174193\n",
      "ditsy             2.163920\n",
      "900               2.159900\n",
      "smoked            2.154484\n",
      "relevance         2.146844\n",
      "expo20            2.130280\n",
      "gd                2.129583\n",
      "quarter           2.127280\n",
      "summarized        2.127201\n",
      "opaque            2.114269\n",
      "1802              2.097430\n",
      "laminate          2.087559\n",
      "availability      2.077975\n",
      "1803              2.061171\n",
      "intensely         2.060218\n",
      "cmd               2.056934\n",
      "devotes           2.052080\n",
      "classwide         2.051857\n",
      "cow               2.041050\n",
      "distributes       2.031179\n",
      "hesitant          2.022855\n",
      "michael           2.020453\n",
      "manditory         2.013229\n",
      "\n",
      "                      0\n",
      "pertaining    -2.982723\n",
      "lawyer        -2.679188\n",
      "preaches      -2.201696\n",
      "hybrid        -1.866316\n",
      "snack         -1.813245\n",
      "purchase      -1.777940\n",
      "smallest      -1.760918\n",
      "breath        -1.741871\n",
      "blog          -1.608357\n",
      "19            -1.561195\n",
      "simulation    -1.561045\n",
      "supplemental  -1.545988\n",
      "resourceful   -1.543320\n",
      "edit          -1.533393\n",
      "flowing       -1.506764\n",
      "span          -1.499916\n",
      "calendar      -1.452923\n",
      "intersting    -1.450783\n",
      "retired       -1.441156\n",
      "caution       -1.436575\n",
      "vital         -1.427760\n",
      "retained      -1.412520\n",
      "flake         -1.411136\n",
      "presession    -1.406694\n",
      "plug          -1.395686\n",
      "aggressive    -1.392595\n",
      "awfully       -1.380010\n",
      "blessing      -1.375631\n",
      "replying      -1.349035\n",
      "informational -1.339471\n",
      "211           -1.338352\n",
      "opening       -1.334789\n",
      "slideshows    -1.332104\n",
      "electronic    -1.330542\n",
      "liking        -1.325351\n",
      "incorrectly   -1.317006\n",
      "copied        -1.312047\n",
      "survival      -1.307906\n",
      "replace       -1.306604\n",
      "biterm        -1.305424\n",
      "forgetful     -1.281587\n",
      "canvas        -1.264904\n",
      "softly        -1.251653\n",
      "snarky        -1.226757\n",
      "hoot          -1.223222\n",
      "awkward       -1.220649\n",
      "choir         -1.218514\n",
      "appealing     -1.207050\n",
      "511           -1.196633\n",
      "illness       -1.194306\n"
     ]
    }
   ],
   "source": [
    "features = ['_'.join(s.split()) for s in vectorizerc.get_feature_names()]\n",
    "coefs_df = pd.DataFrame.from_records(lr.coef_, columns=features)\n",
    "coefs_df\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=False).head(50))\n",
    "print()\n",
    "print(coefs_df.T.sort_values(by=[0], ascending=True).head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee523e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df.to_csv(\"coef_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06bdd1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8096923023554307\n",
      "testing accuracy: 0.7964408367155792\n"
     ]
    }
   ],
   "source": [
    "# classifier - \n",
    "lr = LogisticRegression(random_state=0, penalty = 'l1', solver = 'saga')\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_tf,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_tf)\n",
    "test_preds = lr.predict(X_test_tf)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3452b",
   "metadata": {},
   "source": [
    "### Check difference between Lasso and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd58c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8515801158636\n",
      "testing accuracy: 0.7936830055156624\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# classifier\n",
    "lr = LogisticRegression(random_state=0, penalty = 'l2', solver = 'saga', max_iter=4000)\n",
    "\n",
    "#training\n",
    "lr.fit(X_train_c,y_train)\n",
    "\n",
    "#testing\n",
    "train_preds = lr.predict(X_train_c)\n",
    "test_preds = lr.predict(X_test_c)\n",
    "print(\"training accuracy:\", np.mean([(train_preds==y_train)]))\n",
    "print(\"testing accuracy:\", np.mean([(test_preds==y_test)]))\n",
    "\n",
    "end = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
